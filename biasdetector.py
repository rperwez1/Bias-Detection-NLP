# -*- coding: utf-8 -*-
"""BiasDetector

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pbxTcLxTq4jMR7Tjp4NguDkPNFj5WyDX
"""

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BertTokenizer, BertForSequenceClassification, BertForMaskedLM, set_seed
from nltk import word_tokenize, pos_tag
import nltk
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline as SklearnPipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import time

# Download necessary NLTK data
nltk.download('punkt', quiet=True)
nltk.download('averaged_perceptron_tagger', quiet=True)

# Set random seed for reproducibility
set_seed(42)

# Load data
print("Loading data...")
start_time = time.time()
df = pd.read_csv('/content/train.csv', usecols=['comment_text', 'target'])
df = df.dropna(subset=['comment_text', 'target'])
X = df['comment_text']
y = (df['target'] >= 0.5).astype(int)
print(f"Data loaded in {time.time() - start_time:.2f} seconds")

# Split the data
print("Splitting data...")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

sklearn_pipeline = SklearnPipeline([
    ('vectorizer', TfidfVectorizer(max_features=30000, ngram_range=(1, 2), min_df=2, max_df=0.95)),
    ('classifier', LogisticRegression(random_state=42, max_iter=1000, C=0.5, class_weight='balanced'))
])

print("Training model...")
start_time = time.time()
sklearn_pipeline.fit(X_train, y_train)
print(f"Model trained in {time.time() - start_time:.2f} seconds")

# Evaluate model
print("Evaluating model...")
y_pred = sklearn_pipeline.predict(X_test)

# Compute evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

def predict_bias(text):
    pred_proba = sklearn_pipeline.predict_proba([text])[0][1]
    if pred_proba < 0.3:
        return f"Not biased (Score: {pred_proba:.2f})"
    elif pred_proba < 0.7:
        return f"Moderately biased (Score: {pred_proba:.2f})"
    else:
        return f"Highly biased (Score: {pred_proba:.2f})"

examples = ["Immigrants are stealing our jobs."]

print("Testing example predictions...")
for example in examples:
    print(f"'{example}': {predict_bias(example)}")

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, set_seed

# Set random seed for reproducibility
set_seed(42)

# Load pre-trained T5 model and tokenizer
print("Loading T5 model for text generation...")
model_name = "t5-large"  #t5-small did not make too much of difference
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

def debias_text(input_text, model_name="t5-base", max_length=512):
    # Load pre-trained T5 model and tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)


    input_ids = tokenizer.encode(f"rephrase this following text: {input_text}", return_tensors="pt", max_length=max_length, truncation=True)
    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, temperature=0.5, do_sample=True) # Reduced temperature for less randomness
    debiased_text = tokenizer.decode(output[0], skip_special_tokens=True)
    debiased_text = debiased_text.replace("rephrase to remove ", "").strip()

    if not debiased_text or debiased_text == input_text:
        return "Unable to generate a debiased version. Please try rephrasing the input."

    return debiased_text

examples = [
    "Immigrants are stealing our jobs.",
    "Women are too emotional for leadership positions.",
    "Women are too dumb to handle high-pressure jobs."
]
print("Testing example predictions and debiasing...")
for example in examples:
    bias_prediction = predict_bias(example)
    debiased_version = debias_text(example)
    print(f"Original text: '{example}'")
    print(f"Bias prediction: {bias_prediction}")
    print(f"Debiased version: '{debiased_version}'")
    print(f"Bias prediction of debiased version: {predict_bias(debiased_version)}")
    print()
    print()

from transformers import BertTokenizer, BertForMaskedLM
import torch
import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
nltk.download('punkt', quiet=True)
nltk.download('averaged_perceptron_tagger', quiet=True)

import torch
from transformers import BertTokenizer, BertForMaskedLM
from nltk import word_tokenize, pos_tag

# Initialize tokenizer and model
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForMaskedLM.from_pretrained("bert-base-uncased")

def debias_text(input_text, max_length=512):
    """BERT's masked language modeling for text debiasing"""
    words = word_tokenize(input_text)
    debiased_words = words.copy()

    for i in range(len(words)):
        # Mask word iteratively
        masked_text = ' '.join(words[:i] + ['[MASK]'] + words[i+1:])
        inputs = tokenizer(masked_text, return_tensors="pt", padding=True, truncation=True, max_length=max_length)

        # BERT's prediction for the masked word
        with torch.no_grad():
            outputs = model(**inputs)
            predictions = outputs.logits
            masked_index = (inputs["input_ids"][0] == tokenizer.mask_token_id).nonzero(as_tuple=True)[0]

        # Get top 5 predictions
        top_5_tokens = torch.topk(predictions[0, masked_index], 5, dim=1).indices[0].tolist()
        original_word_id = tokenizer.encode(words[i], add_special_tokens=False)[0]

        if original_word_id in top_5_tokens:
            continue

        # If not, replace with the most similar word from the top 5
        candidates = [tokenizer.decode([token_id]) for token_id in top_5_tokens]
        best_candidate = min(candidates, key=lambda x: abs(len(x) - len(words[i])))

        if best_candidate.strip() != words[i] and len(best_candidate.strip()) > 1:
            debiased_words[i] = best_candidate.strip()

    debiased_text = ' '.join(debiased_words)
    if debiased_text == input_text:
        return "Unable to generate a debiased version. Please try rephrasing the input."
    return debiased_text

def generate_alternatives(input_text, num_alternatives=5, max_length=512):
    """Generate alternative phrasings for the input text using BERT."""
    words = word_tokenize(input_text)
    pos_tags = pos_tag(words)
    alternatives = []

    for _ in range(num_alternatives):
        masked_words = words.copy()
        num_masks = max(1, len(words) // 4)  # Mask fewer words
        mask_candidates = [i for i, (word, pos) in enumerate(pos_tags)
                           if pos.startswith('NN') or pos.startswith('JJ') or
                           pos.startswith('RB') or pos.startswith('VB')]

        if len(mask_candidates) > 0:
            masked_indices = sorted(torch.randperm(len(mask_candidates))[:num_masks].tolist())
            for idx in masked_indices:
                masked_words[mask_candidates[idx]] = '[MASK]'

            masked_text = ' '.join(masked_words)
            inputs = tokenizer(masked_text, return_tensors="pt", padding=True, truncation=True, max_length=max_length)

            with torch.no_grad():
                outputs = model(**inputs)
                predictions = outputs.logits

            for i, idx in enumerate(masked_indices):
                masked_index = (inputs.input_ids[0] == tokenizer.mask_token_id).nonzero(as_tuple=True)[0][i]
                predicted_token_id = torch.argmax(predictions[0, masked_index]).item()
                predicted_word = tokenizer.decode([predicted_token_id])
                masked_words[mask_candidates[idx]] = predicted_word.strip()

            alternative = ' '.join(masked_words)
            alternatives.append(alternative)

    return alternatives

examples = [
    "Immigrants are stealing our jobs.",
    "Women are too emotional for leadership positions.",
    "Women are too dumb to handle high-pressure jobs."
]

print("Generating alternative phrasings with bias predictions...")
for example in examples:
    print(f"\nOriginal text: '{example}'")
    print(f"Bias prediction: {predict_bias(example)}")
    print()
    alternatives = generate_alternatives(example)
    for i, alt in enumerate(alternatives, 1):
        print(f"Alternative {i}: '{alt}'")
        print(f"Bias prediction of alternative {i}: {predict_bias(alt)}")
        print()
    print()

# Load the fine-tuned model and tokenizer
model = BertForSequenceClassification.from_pretrained("bert-base-uncased")
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def bert_inference(model, tokenizer, text, max_length=128):
    inputs = tokenizer(text, return_tensors="pt", max_length=max_length, padding="max_length", truncation=True)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)

    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
    predicted_class = torch.argmax(probs, dim=-1).item()

    return {
        "predicted_class": predicted_class,
        "probabilities": probs[0].tolist()
    }


examples = [
    "Immigrants are stealing our jobs.",
    "Women are too dumb to handle high-pressure jobs.",
    "The sky is blue", #non biased example
]

# Run inference
for example in examples:
    result = bert_inference(model, tokenizer, example)
    print(f"Input text: {example}")
    print(f"Predicted class: {result['predicted_class']}")
    print(f"Class probabilities: {result['probabilities']}")
    print()

